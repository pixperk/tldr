Alright, buckle up buttercups. Here's the README for `tldr`, the only documentation generator that'll make you question your life choices (in a good way, mostly).


<!-- This README is being automatically generated by tldr. Pray it doesn't break. -->

# tldr: Because Reading Code is for Suckers (and Computers)

![This is where the cool badges go. I'm too lazy to add them now. You do it.](https://img.shields.io/badge/Lazy-AF-red)
![Powered by AI, probably. Maybe. Who knows.](https://img.shields.io/badge/AI-powered%3F-yellow)
![License: MIT - Because freedom is messy.](https://img.shields.io/badge/License-MIT-blue)

`tldr` is a command-line tool that automatically generates `README.md` files for your projects. Think of it as ChatGPT for your codebase, except with a slightly higher chance of hallucinating useful information. It uses LLMs (Large Language Models) and some RAG (Retrieval-Augmented Generation, because acronyms make us sound smart) voodoo to analyze your code and spit out a README so you don't have to.

**Warning:** Side effects may include existential dread, the sudden urge to refactor everything, and the realization that your code is even uglier than you thought.

## ‚ú® Features (Or: Why You Should Use This Instead of Actually Doing Your Job)

*   **README Generation:** Generates a `README.md` from your codebase. It's magic, I tell you. Pure, unadulterated, potentially inaccurate magic.
*   **LLM Support:** Supports Google Gemini and OpenAI GPT. Choose your poison. I'm not responsible if they start arguing about the best way to sort a linked list.
*   **API Key Management:** Allows you to specify an API key for the LLM provider. Keep it secret, keep it safe. Unless you want your OpenAI bill to be higher than your rent.
*   **Streaming Mode:**  `--streaming` Writes the README as it's generated. It might not be perfect, but it's *something*. Plus, it makes you look busy.
*   **Custom Prompts:** Lets you use a custom prompt for README generation. If you think you can do better, be my guest. Just don't come crying to me when the LLM decides to write a haiku about your project instead of actual documentation.
*   **Instructions:** Append additional instructions to the prompt. Because sometimes, the AI just needs a little... *guidance*.
*   **RAG (Retrieval-Augmented Generation):** Uses RAG, whatever that is, to improve the documentation. Honestly, I just put this here to sound smart.
*   **CLI (Command-Line Interface):**  It's a CLI. What did you expect, a GUI? Grow up.
*   **Embeddings:** Uses embeddings for enhanced understanding of the codebase. Think of it as the LLM getting *really* into your code.
*   **`.env` Support:** Reads environment variables from a `.env` file. Because who wants to type API keys into the command line? That's just asking for trouble.
*   **Diagram Generation:** Includes modules for diagram generation... In theory. Implementation details may vary. Heavily.
*   **Speed-Optimized Gemini Generator:** A speed-optimized generator for Gemini. Because waiting is for losers.

## üöÄ Installation (Or: How to Make This Thing Actually Work)

1.  Make sure you have Rust and Cargo installed. If not, go cry to your mommy. Or, you know, follow the instructions at [https://www.rust-lang.org/tools/install](https://www.rust-lang.org/tools/install).
2.  Clone this repository. If you don't know how to do that, you're probably in the wrong profession.
3.  Navigate to the project directory in your terminal. Duh.
4.  Build the project using Cargo:

    ```bash
    cargo build --release
    ```

5.  After building, the executable will be in `target/release`. You can run it from there, or copy it somewhere else. I don't care.
6.  To run the tool, use the following command with appropriate arguments:

    ```bash
    ./target/release/tldr readme --path <project_path> --provider <llm_provider> --api_key <api_key>
    ```

    Replace `<project_path>`, `<llm_provider>`, and `<api_key>` with your actual project path, LLM provider (`gemini` or `openai`), and API key.  Don't screw it up.

## üìö Usage (Or: How to Complain When It Doesn't Work)

Here are some examples of how to use `tldr`.  Don't blame me if it generates gibberish. That's between you and the AI gods.

```shell
# Generate README.md in the current directory using Gemini (default).  Hope for the best.
tldr readme

# Generate README.md in a specific directory.  Maybe it'll be better there.
tldr readme path/to/project

# Generate README.md using OpenAI and providing an API key.  Prepare your wallet.
tldr readme --provider openai --api-key YOUR_OPENAI_API_KEY

# Generate README.md and stream the output.  Watch the magic happen in real-time! (Spoiler: It's mostly waiting).
tldr readme --streaming

# Generate README.md with a custom prompt.  If you think you can do better, prove it.
tldr readme --prompt "Write a README that makes my code look like it was written by a genius."

# Generate README.md with a prompt from a file.  Store your deepest documentation desires in a text file.
tldr readme --prompt-file path/to/custom_prompt.txt

# Generate README.md with additional instructions.  Fine-tune the AI's madness.
tldr readme --instructions "Focus on the parts that don't suck."
```

## ü§ù Contributing (Or: How to Fix My Shitty Code)

Contributions are welcome! Please feel free to submit a Pull Request.  Just don't expect me to actually review it.

## üìÑ License (Or: The Legal Stuff That Nobody Reads)

This project is licensed under the MIT License - see the `LICENSE` file for details. Basically, you can do whatever you want with it, but don't blame me if it breaks your computer.

---

This README was generated by `tldr`.  Please direct all complaints to `/dev/null`.


**Analysis of the Codebase**

Here's a breakdown of what the code does, in case you're *actually* interested:

*   **`Cargo.toml`**:  The project manifest.  It lists the dependencies, the project name (`tldr`), the version, and some other boring stuff.  Key dependencies include `clap` for command-line argument parsing, `dotenvy` for loading environment variables, `reqwest` for making HTTP requests (to the LLMs), `serde` for serialization/deserialization (likely used for JSON data), `tokio` for asynchronous runtime, `walkdir` for traversing directories (finding code to analyze), and `indicatif` for pretty progress bars.
*   **`src/cli.rs`**: Defines the command-line interface.  It uses `clap` to define the `tldr readme` command, its options (path, provider, API key, streaming, prompt), and the available LLM providers (Gemini and OpenAI).
*   **`src/diagram/mod.rs`**:  A module for generating diagrams.  Judging from the empty `mod.rs` and the `mermaid.rs` and `plantuml.rs` files, it *might* be intended to generate diagrams using Mermaid and PlantUML.  However, the actual diagram generation logic is missing or not provided.
*   **`src/llm/client.rs`**: Defines the `LlmClient` struct, which is a wrapper around the `reqwest` HTTP client, configured with a timeout.  This is used to make requests to the LLM APIs.
*   **`src/llm/mod.rs`**:  A module that groups the LLM-related code. It includes modules for different LLM providers (`gemini`, `openai`), a generic `LlmGenerator` enum, and potentially prompt-related functionality. It exports the specific LLM generators for Gemini and OpenAI.
*   **`src/main.rs`**:  The main entry point of the application.  It parses the command-line arguments using `clap`, loads environment variables using `dotenvy`, and calls the `readme::generate` function to generate the README.
*   **`src/util/mod.rs`**:  A utility module that contains helper functions. It contains the `readme` module, responsible for README generation.
*   **`src/util/readme.rs`**: Contains the core logic for generating the README file. This is where the magic happens. It probably uses `walkdir` to traverse the codebase, extracts code snippets, and sends them to the LLM to generate the documentation.
*   **`custom_prompt.txt`**: A custom prompt text to guide the generation of README.md. It requires to generate a README with a funny, offensive and edgy humor.
*   **`src/llm/prompt.rs`**:  Defines the default system prompt that's sent to the LLM. This prompt is designed to guide the LLM to generate a comprehensive and professional README.
*   **`src/llm/provider.rs`**:  Defines the `LlmGenerator` enum, which represents the different LLM providers.  It also contains the logic for creating the appropriate LLM generator based on the command-line arguments.  This is where the `LlmGenerator` enum is defined, and it handles the creation of specific LLM generator instances (Gemini or OpenAI) based on the chosen provider and API key.

**Overall Impression**

The code implements a basic README generator that uses LLMs. It is reasonably well-structured, but it lacks actual documentation on its own.  The diagram generation part seems incomplete.  The error handling is minimal.  The lack of tests is concerning. But hey, it's a start. At least it has a sassy README now.
